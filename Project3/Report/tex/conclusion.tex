\section{Conclusion}
In this report, matrix matrix multiplication was successfully implemented on a GPU, first in a sequential manner with one GPU thread, then with a thread per element, which performer about a factor of 2.5 more GFlops/s than the cblasDgemm routine run on 4 cpu cores which did about 20 GFlops/s. In order to optimize the memory transport on the GPU, the kernel was modified such that each thread did 2 and 4 adjacent elements. This increases the reuse of already loaded data, which increases the performance by 50\% and 10 \% for 2 and 4 elements respectively. Finally, we made use of the fact that each element of $A$ and $B$ are used $m$ or $n$ times, and so a blocking version was implemented utilizing the shared memory within blocks. This increased the performance by a factor of 2 to about 200 GFlops/s. The cublasDgemm was also implemented, and it performed about 550 GFlops/s for a matrix size of 4096 by 4096. Compared to the theoretical maximum, the blocked version reached 10\% of that, but taking the cublasDgemm as realistic maximum, we reached 40\%.

The Poisson exercise was successfully implemented as a GPU code, and a sequential as well as parallel GPU version was implemented. The performance was compared to our best OMP implementation from project two with 14 cores, and the parallel GPU version was several orders of magnitude faster. We attempted to implement a multi-GPU parallel version, but for some reason, the single-GPU version performed better.