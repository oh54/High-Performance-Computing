\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Selected specifications of the Tesla K40c GPU.\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:gpuspecs}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Assignment}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Matrix matrix multiplication problem}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}GPU1}{2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}gpu1 function with the cuda framework. The kernel \texttt  {cudaSeq} is identical to the mkn permutaion from project 1.}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}GPU2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The \texttt  {gpu1} function compared to the cblas DGEMM run on 4 cores.\relax }}{3}}
\newlabel{fig:gpu1_DGEMM}{{1}{3}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Code sample of the naive implementation with one thread per element in $C$.}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance of the \texttt  {gpu2} function and the cblas DGEMM for different matrix sizes.\relax }}{4}}
\newlabel{fig:gpu2_DGEMM}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}GPU3}{5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}The source code for texttt{gpu3}. The $l$ loop is the outer loop to increase memory re-usage.}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance of \texttt  {gpu2}, \texttt  {gpu3} and the cblasDgemm functions for increasing matrix sizes.\relax }}{6}}
\newlabel{fig:gpu3}{{3}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}GPU4}{6}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}The source code for texttt{gpu4}. The $l$ loop is the outer loop to increase memory re-usage.}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}GPU5}{7}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5}Source code for the shared memory kernel}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance of the gpu functions and the cblasDgemm as a function of matrix size.\relax }}{8}}
\newlabel{fig:gpu4}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Performance of the gpu functions and the cblasDgemm library function. \texttt  {gpu5} is seen to be roughly a factor of 2 faster than \texttt  {gpu4}.\relax }}{9}}
\newlabel{fig:gpu5}{{5}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}GPULIB}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Performance of gpu2 to gpu5, the cublas function, and the cblas function versus the matrix size.\relax }}{10}}
\newlabel{fig:gpuALL}{{6}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Poisson problem}{10}}
