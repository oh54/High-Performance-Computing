\section{OpenMP Gauss-Seidel}
The problem with the Gauss-Seidel smoother is, that it is built around internal data dependencies. It is therefore difficult to implement simple parallelized techniques, since the communication cost between the different threads would be really high. The Jacobi method on the other hand uses another matrix, the u old, which takes up memory space, but it is much easier to feed the data to the threads and finish the computation step. Since implementing a parallelization of the Gauss-Seidel smoothening is more difficult, the Jacobi method or alternatives have been more widely used for multigrid problems.\\

There does however exist modifications to the natural/lexographical version of the Gauss-Seidel smoother, which then seeks parallize using different methods, i.e. blocking. The article by D. Wallin, H. L\"{o}f, E. Hagersten and S. Holmgren\footnote{Dan Wallin, Henrik L\"{o}f, Erik Hagersten and Sverker Holmgren, \textit{Multigrid and GaussSeidel
Smoothers Revisited:
Parallelization on Chip Multiprocessors}, ICS '06 Page 145-155, ACM NY USA, 2006-06-28}  give insight to the complications of the subject and describe both the standard red-black method and their new temporally blocked natural Geiss-Seidel smoother. The completely natural Gauss-Seidel is a single sweep for each step/iteration, and therefore the data is interchanged. The red-black ordering is based on splitting each iteration step up in sweeps, which are fully data parallel. The grid is split into sets of even and odd gridpoints, even points have data dependencies to odd points and vice versa. However, the points do not have dependence to points of the same type. This means a loop over points of the same type can be done in parallel. In practice our initial approach would thus likely be to have a loop over one type of points, then a barrier and then a loop over points of the other type.

Wallin et al succeeded in improving the efficiency up to 40\% compared to some other Gauss-Seidel parallelized smoothers, but it does however depend much on the system and problem at hand.

There exist several possible implementations of the Gauss-Seidel smoother, but it is an evolving field and there exist different approaches. The article by H. Courtecuisse and J. Allard\footnote{Hadrien Courtecuisse, J\'{e}r\'{e}mie Allard, \textit{Parallel Dense Gauss-Seidel Algorithm on Many-Core Processors}, High Performance Computing and Communications, 1th IEEE Conference 2009, IEEE, 2009-06-25} comes with different approaches to the same issue. They present a new parallel dense Gauss-Seidel algorithm, which is based on atomic update counter, which stores an integer counter in shared memory describing the processed blocks. This depends on the system used, but if it is possible on the system, then after an initial delay, the algorithm will get up to speed without facing the huge communication costs. The article further analyzes this algorithm in depth and its performance across different problem parameters and hardware systems.\\
