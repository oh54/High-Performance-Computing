\section{OpenMP Gauss-Seidel}
The problem with the Gauss-Seidel smoother is, that it is built around internal data dependencies. It is therefore difficult to implement simple parallelized techniques, since the communication cost between the different threads would be really high. The Jacobi method on the other hand uses another matrix, the u old, which takes up memory space, but it is much easier to feed the data to the threads and finish the computation step. Since implementing a parallelization of the Gauss-Seidel smoothening is more difficult, the Jacobi method or alternatives have been more widely used for multigrid problems.\\

There does however exist modifications to the natural/lexographical version of the Gauss-Seidel smoother, which then seeks parallize using different methods, i.e. blocking. The article by D. Wallin, H. L\"{o}f, E. Hagersten and S. Holmgren\footnote{Dan Wallin, Henrik L\"{o}f, Erik Hagersten and Sverker Holmgren, \textit{Multigrid and GaussSeidel
Smoothers Revisited:
Parallelization on Chip Multiprocessors}, ICS '06 Page 145-155, ACM NY USA, 2006-06-28}  give insight to the complications of the subject and describe both the standard red-black method and their new temporally blocked natural Geiss-Seidel smoother. The completely natural Gauss-Seidel is a single sweep for each step/iteration, and therefore the data is interchanged. The red-black ordering is based on splitting each iteration step up in sweeps, which are fully data parallel. The multigrid is then split up in a set of even and odd gridpoints, and obviously even and odd points do not have any data dependency on each other. Each sweep then updates them independently, without the need of ongoing communication between the threads. Their own implementation does blocking as well, but it is not temporal and not strictly like the red-black ordering, which gives the advantage of better cache efficiency for larger matrices compared to the red-black method. Through their work, they succeeded in improving the efficiency up to 40\% compared to some other Gauss-Seidel parallelized smoothers, but it does however depend much on the system and problem at hand.\\

There do exist several possible implementations of the Gauss-Seidel smoother, but it is an evolving field and there exist different approaches. The article by H. Courtecuisse and J. Allard\footnote{Hadrien Courtecuisse, J\'{e}r\'{e}mie Allard, \textit{Parallel Dense Gauss-Seidel Algorithm on Many-Core Processors}, High Performance Computing and Communications, 1th IEEE Conference 2009, IEEE, 2009-06-25} comes with different approaches to the same issue. They present a new parallel dense Gauss-Seidel algorithmn, which is based on atomic update counter, which stores an integer counter in shared memory describing the processed blocks. This depends on the system used, but if it is possible on the system, then after an initial delay, the algorithm will get up to speed without facing the huge communication costs. The article further analyzes this algorithm in depth and its performance across different problem parameters and hardware systems.\\
If we had to implement a parallel Gauss-Seidel algorithm, we would begin with the red-black ordering, since it seems easier to implement and understand, atleast initially.